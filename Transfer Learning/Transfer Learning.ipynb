{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with Keras\n",
    "\n",
    "In this tutorial we'll look at how to utilize Keras and a pretrained model to predict a custom object. This is useful for when you don't want (or don't have the possibility/time) to train a model from scratch and you have a problem that is similar to one with an already existing solution.\n",
    "\n",
    "The following code is a complete solution from loading a train set to creating train and validation set generators to predicting and checking performance.\n",
    "\n",
    "We will load an existing model into Keras, remove its final layer and train it on our custom dataset to get image predictions without creating and training a whole network within minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to modify some path variables and your results might not completely match mine due to how training neural nets work, but if you have Keras installed, it should work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load the packages we're going to need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy.random import permutation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Input, Lambda, Dense\n",
    "from keras.models import load_model, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use 2 helper functions, read an image into a neural net-friendly format and another one to do this for all images in a folder. We're going to stick to the default resolution of the VGG19 model for ease. Thankfully we have the helper function `preprocess_input` available to us, preparing the input to be compatible with Keras/VGG19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_path, expand_dims = False, target_size=(224, 224)):\n",
    "    img = image.load_img(img_path, target_size)\n",
    "    x = image.img_to_array(img)\n",
    "    if expand_dims == True: \n",
    "        x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_images(category, target_size, path = \"./\"):\n",
    "    images = []\n",
    "    i = 0\n",
    "    for file in sorted(os.listdir(path+category)):\n",
    "        if file[0] != \".\":\n",
    "            img_path = path+category+\"/\"+file\n",
    "            x = process_image(img_path, target_size)\n",
    "            images.append(x)\n",
    "    images = np.asarray(images)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gathered 10 random images of cats and 10 random images of lightbulbs off Google. Some examples are: \n",
    "\n",
    "|Cat 1 |Cat 2|\n",
    "|---|----|\n",
    "|![cats/cat1.jpg](cats/cat1.jpg)|![cats/cat2.jpg](cats/cat2.jpg)|\n",
    "\n",
    "|Lightbulb 1 |Lightbulb 2 |\n",
    "|---|----|\n",
    "|![lightbulbs/lightbulb1.jpeg](lightbulbs/lightbulb1.jpeg)|![lightbulbs/lightbulb2.jpg](lightbulbs/lightbulb3.jpg)|\n",
    "\n",
    "The original VGG19 model was trained to predict 1000 categories (in accordance with the [ImageNet Challenge](http://image-net.org/challenges/LSVRC/)) including cats, but we're going to utilise transfer learning to modify it and predict 2 categories: cats and lightbulbs. While cats are a part of the original 1000 categories, lightbulbs are not. With the power of transfer learning, however, we are able to introduce and reliably predict new categories based on this pretrained model.\n",
    "\n",
    "Let's start with reading the images into Python variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = parse_images(\"cats\", target_size)\n",
    "bulbs = parse_images(\"lightbulbs\", target_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're marking cats as category 0 and bulbs as category 1, and then combining the images and the categories into a single variable each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_y = np.zeros((len(cats)))\n",
    "bulbs_y = np.ones((len(cats)))\n",
    "\n",
    "X = np.vstack((cats, bulbs))\n",
    "y = np.concatenate((cats_y, bulbs_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 224, 224, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our input `X` has the shape of (20, 224, 224, 3) - we have 20 images, all with the resolution of 224x224 and 3 channels each (RGB color coding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variables (the classes) for the VGG19 model have to be [one-hot encoded](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f), so let's transform our `y`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = np.zeros((len(y), 2))\n",
    "train[np.arange(len(y)), y.astype(int)] = 1\n",
    "y = train\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the first column ha the value 1 if the image is a cat and 0 if it isn't. The same applies to column 2 with the image being a lightbulb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = permutation(len(y))\n",
    "X = X[perm]\n",
    "y = y[perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the input in the format we require. Let's load the pre-trained VGG19 model! We are specifying that we want to use the trained model with the weights for the imagenet dataset. We could also load the model without weights initialized, but that would mean we'd need to retrain the whole model from scratch, and that's the opposite of what we're trying to do with transfer learning. You might get some warnings, but disregard them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 16:49:51.822905 140736037671808 deprecation_wrapper.py:119] From /Users/csanadpoda/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0827 16:49:51.852724 140736037671808 deprecation_wrapper.py:119] From /Users/csanadpoda/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0827 16:49:51.857797 140736037671808 deprecation_wrapper.py:119] From /Users/csanadpoda/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0827 16:49:51.901552 140736037671808 deprecation_wrapper.py:119] From /Users/csanadpoda/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0827 16:49:55.114920 140736037671808 deprecation_wrapper.py:119] From /Users/csanadpoda/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0827 16:49:55.116116 140736037671808 deprecation_wrapper.py:119] From /Users/csanadpoda/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vgg19_model = VGG19(include_top=True, weights='imagenet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following command to see the full architecture/summary of the model. As you can see, our final dense layer contains 1000 nodes to predict the 1000 categories. We're going to replace that one specifically and keep everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 143,667,240\n",
      "Trainable params: 143,667,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg19_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're removing the final layer (alternatively you could load the model without this final layer by using `include_top=False`, I just wanted you to see the full original architecture of the network). We then create a new dense laayer with 2 nodes (for our two categories: cats and lightbulbs) and specify the inputs and outputs for our new custom model. Then we check the summary: we can see that the architecture is completely the same except for the final layer (the one responsible for predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 139,578,434\n",
      "Trainable params: 139,578,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg19_model.layers.pop()\n",
    "new_layer = Dense(2, activation='softmax', name='predictions')\n",
    "inp = vgg19_model.input\n",
    "out = new_layer(vgg19_model.layers[-1].output)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the layers and all the parameters are trainable by default, so we need to freeze all the layers except the last one, since we already have the trained weights for those. This is the main idea behind transfer learning: we keep the pretrained model as we believe it is already very good at extracting the features that are important for (image category) prediction, and we just insert and train our own final layer to predict our desired categories based on the features the pre-trained network already extracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x1a2cb68748> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2cb53278> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2cabfa20> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x1a2cb68b38> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f61e6a0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f649fd0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x1a2f661f28> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f661c18> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f6924a8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f6aecf8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f6e3630> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x1a2f6ffb38> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f6ff828> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f72e710> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f746e48> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f760eb8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x1a2f7986d8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f778c18> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f7b4c88> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f7c9f60> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x1a2f7dcd68> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x1a2f813d30> False\n",
      "<keras.layers.core.Flatten object at 0x1a2f813ac8> False\n",
      "<keras.layers.core.Dense object at 0x1a2f8489b0> False\n",
      "<keras.layers.core.Dense object at 0x1a2f864780> False\n",
      "<keras.layers.core.Dense object at 0x1a510ca278> True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all the layers are now frozen, so when we're trainings the weights won't get updated, except for the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 139,578,434\n",
      "Trainable params: 8,194\n",
      "Non-trainable params: 139,570,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, out of the total 139,578,434 parameters we're only going to train 8,194 and *still* get great results - and training less parameters means less computing power needed and less time spent training! \n",
    "\n",
    "Let's split our dataset into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only have a tiny dataset, it's recommended we use image data generators. This allows us to have a greater dataset - these generators take our existing dataset but apply a range of transformations to our images (rotate them, shift the width and/or height, flip them horizontally), so we get a bigger train/test set that we can feed to the neural net which thus becomes more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator()\n",
    " \n",
    "train_generator = train_datagen.flow(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=4)\n",
    "\n",
    "validation_generator = validation_datagen.flow(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to add some callback functions, so we can stop the training early in case we reach a good enough model or can't improve the model anymore. \n",
    "With `EarlyStopping` we monitor the loss and we will stop training if the loss doesn't improve.\n",
    "With `ReduceLROnPlateau` we reduce the learning rate if the model cannot improve anymore with the current learning rate.\n",
    "With `ModelCheckpoint` we create a checkpoint for the model and save it anytime the model performs better than in the previous epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 16:50:20.204329 140736037671808 deprecation_wrapper.py:119] From /Users/csanadpoda/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/3 [====================================] - 6s 2s/step - loss: 0.2869 - acc: 0.6077 - ce: 0.9539\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.31469, saving model to trained_model.h5\n",
      "Epoch 2/100\n",
      "4/3 [====================================] - 6s 1s/step - loss: 0.0325 - acc: 0.9346 - ce: 0.1014\n",
      "\n",
      "Epoch 00002: loss improved from 0.31469 to 0.03827, saving model to trained_model.h5\n",
      "Epoch 3/100\n",
      "4/3 [====================================] - 6s 2s/step - loss: 1.5437e-06 - acc: 1.0000 - ce: 6.4574e-04\n",
      "\n",
      "Epoch 00003: loss improved from 0.03827 to 0.00000, saving model to trained_model.h5\n",
      "Epoch 4/100\n",
      "4/3 [====================================] - 5s 1s/step - loss: 3.4533e-06 - acc: 1.0000 - ce: 0.0010\n",
      "\n",
      "Epoch 00004: loss improved from 0.00000 to 0.00000, saving model to trained_model.h5\n",
      "Epoch 5/100\n",
      "4/3 [====================================] - 5s 1s/step - loss: 4.5189e-07 - acc: 1.0000 - ce: 2.8087e-04\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00005: loss improved from 0.00000 to 0.00000, saving model to trained_model.h5\n",
      "Epoch 6/100\n",
      "4/3 [====================================] - 5s 1s/step - loss: 1.1277e-06 - acc: 1.0000 - ce: 6.0108e-04\n",
      "\n",
      "Epoch 00006: loss improved from 0.00000 to 0.00000, saving model to trained_model.h5\n",
      "Epoch 7/100\n",
      "4/3 [====================================] - 7s 2s/step - loss: 1.0020e-06 - acc: 1.0000 - ce: 4.4036e-04\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.00000\n",
      "Epoch 8/100\n",
      "4/3 [====================================] - 6s 2s/step - loss: 1.0136e-05 - acc: 1.0000 - ce: 0.0018\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.00000\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', min_delta=0.0001, patience=5, verbose=1, mode='auto')\n",
    "# Reducing the Learning Rate if result is not improving. \n",
    "reduce_lr  = ReduceLROnPlateau(monitor='loss',  min_delta=0.0004, patience=2, factor=0.1, min_lr=1e-6,  mode='auto', verbose=1)\n",
    "\n",
    "# define the checkpoint\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizers.Adam(lr=.001, beta_1=0.9, beta_2=0.999, epsilon=0.0000001),\n",
    "              metrics=['acc', 'ce'])\n",
    "\n",
    "filepath = \"trained_model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [early_stop, reduce_lr, checkpoint]\n",
    "\n",
    "# fit the model\n",
    "history = model.fit_generator(train_generator, epochs=100, steps_per_epoch=len(X_train) / 4, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully the training stopped for you after a few epochs just like it stopped for me as the loss could not improve anymore (it reached 0 - this is rarely the case and can signal overfitting in general, keep in mind this is just a demo/tutorial and we're using very few examples - this might impact performance). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line is just so that the output is not printed in the scientific notation for easier reading. You can disregard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model saved, we can load it back with the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = load_model(\"trained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and see how it works on new pictures not in the original dataset! I have a new picture of a cat and a lighbulb:\n",
    "\n",
    "|New cat|New lightbulb|\n",
    "|---|----|\n",
    "|![cat.jpg](cat.jpg)|![lightbulb.jpg](lightbulb.jpg)|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000000, 0.000000]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = './cat.jpg'\n",
    "x = process_image(img_path, expand_dims=True)\n",
    "\n",
    "features = saved_model.predict(x)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.001216, 0.998784]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = './lightbulb.jpg'\n",
    "x = process_image(img_path, expand_dims=True)\n",
    "\n",
    "features = saved_model.predict(x)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how the first column being equal to 1 meant that there's a cat in the picture, and the second column being equal to 1 meant that there's a lightbulb in the picture? The algorithm returns probabilities for each of the classes. For the cat image we can see it was more than 95% sure that there's a cat in the picture and less than 5% sure it was a lightbulb and vice versa for the lightbulb, meaning both of the predictions were correct. We can also use the validation generator to generate multiple images from our test set to make predictions on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(validation_generator, steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what's more important, we can use the validation generator to evaluate our model. Remember, the validation generator returns pictures (and slightly modified version of the pictures) that the neural net hasn't seen before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001838433789089322, 1.0, 0.018193300813436508)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc, ce = model.evaluate_generator(validation_generator, steps=5, verbose=0)\n",
    "loss, acc, ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss for the model is tiny while the accuracy is high, meaning the model performs pretty well on our test set, too! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, you can see the power of transfer learning - with just 10 images per category we've been able to train an algorithm that can reliably predict if there's a cat or a lightbulb in the picture within a few seconds! Do note that this data set is super small and all the pictures are quite similar, and the performance should get worse with more complex tasks, but still you can see how much faster and more efficient transfer learning can be as opposed to training from scratch!\n",
    "\n",
    "Keep in mind our example set is quite homogenous (all lighbulbs are presented on their own on a white background), so the algorithm has a harder time predicting different pictures of lightbulbs. For example, this picture of a lighbulb on a black background already poses a bigger challenge to the model:\n",
    "![lightbullb.jpeg](lightbullb.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.316792, 0.683208]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = './lightbullb.jpeg'\n",
    "x = process_image(img_path, expand_dims=True)\n",
    "\n",
    "features = saved_model.predict(x)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the algorithm in this case is \"only\" 68% (in my case - your model can differ) sure that this is a lightbulb and gives it a 31% chance that it's actually a cat. It still gives the right prediction, so that's quite impressive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the basics, go and experiment! Add more pictures, add more classes, go crazy!\n",
    "\n",
    "If you have any feedback or questions about this tutorial, feel free to reach out to me via [LinkedIn](https://www.linkedin.com/in/csanad-poda)! Happy learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
